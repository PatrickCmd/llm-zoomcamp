{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e46139-ad70-454b-909e-022afdd2ddbd",
   "metadata": {},
   "source": [
    "# Developing RAG Systems with DeepSeek R1 & Ollama\n",
    "\n",
    "## Ollama Setup\n",
    "- https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97\n",
    "- https://apidog.com/blog/rag-deepseek-r1-ollama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eac9d43-4235-40f0-b072-c8d067c7935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d0085c-7a6e-4b5b-bf5d-22c494143e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['nohup', 'ollama', 'serve']>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run the command as a background process\n",
    "subprocess.Popen([\"nohup\", \"ollama\", \"serve\"], stdout=open(\"nohup.out\", \"w\"), stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef9abe1-7fa2-4ce6-94fe-b451154fb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!ollama pull 'deepseek-r1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0baab8d7-c3c8-4916-a965-878037770d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!ollama pull 'deepseek-r1:1.5b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9ea31b-0f1d-457b-b3fc-2a77f5548415",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0824da1-6d11-46c8-82e9-93a9afec9441",
   "metadata": {},
   "source": [
    "## DeepSeek R1 Model Variants\n",
    "DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the 1.5B model for lightweight RAG applications.\n",
    "\n",
    "```sh\n",
    "ollama run deepseek-r1:1.5b\n",
    "```\n",
    "\n",
    "## Step-by-Step Guide to Building the RAG Pipeline\n",
    "### Step 1: Import Libraries\n",
    "We’ll use:\n",
    "\n",
    "- [LangChain](https://github.com/langchain-ai/langchain) for document processing and retrieval.\n",
    "- [Streamlit](https://streamlit.io/) for the user-friendly web interface.\n",
    "\n",
    "![images](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eiQWHXXZgS0pHdi-.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1076ef2e-0503-45dc-9bf9-8bc5bd61ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qq docling docling-core langchain_experimental langchain langchain-text-splitters langchain-huggingface langchain-chroma langchain-groq langchain-ollama langchain-openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6015a839-3349-4077-885f-904059281ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q streamlit\n",
    "!pip install -q pdfplumber\n",
    "!pip install -q faiss-gpu faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5006dce3-c7fd-4ddc-a51b-5bfd7d30cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st  \n",
    "from langchain_community.document_loaders import PDFPlumberLoader  \n",
    "from langchain_experimental.text_splitter import SemanticChunker  \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  \n",
    "from langchain_community.vectorstores import FAISS  \n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf80181-dfed-44fe-a09d-3448abd0fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF text  \n",
    "loader = PDFPlumberLoader(\"./documents/2309.15217v1.pdf\")  \n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf5d17d8-913f-466a-9c98-9f974460dabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a95eb99-3551-4f13-9c4a-88e2d2e1dbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './documents/2309.15217v1.pdf', 'file_path': './documents/2309.15217v1.pdf', 'page': 0, 'total_pages': 8, 'Author': '', 'CreationDate': 'D:20230928011700Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20230928011700Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='RAGAS: Automated Evaluation of Retrieval Augmented Generation\\nShahulEs†,JithinJames†,LuisEspinosa-Anke∗♢,StevenSchockaert∗\\n†ExplodingGradients\\n∗CardiffNLP,CardiffUniversity,UnitedKingdom\\n♢AMPLYFI,UnitedKingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract struggletomemoriseknowledgethatisonlyrarely\\nmentioned in the training corpus (Kandpal et al.,\\nWeintroduceRAGAS(RetrievalAugmented\\n2022;Mallenetal.,2023). Thestandardsolution\\nGeneration Assessment), a framework for\\nto these issues is to rely on Retrieval Augmented\\nreference-free evaluation of Retrieval Aug-\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\nmented Generation (RAG) pipelines. RAG\\n2020; Guu et al., 2020). Answering a question\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide then essentially involves retrieving relevant pas-\\nLLMswithknowledgefromareferencetextual sages from a corpus and feeding these passages,\\ndatabase,whichenablesthemtoactasanatu- alongwiththeoriginalquestion,totheLM.While\\nrallanguagelayerbetweenauserandtextual initial approaches relied on specialised LMs for\\ndatabases,reducingtheriskofhallucinations.\\nretrieval-augmentedlanguagemodelling(Khandel-\\nEvaluatingRAGarchitecturesis,however,chal-\\nwaletal.,2020;Borgeaudetal.,2022),recentwork\\nlengingbecausethereareseveraldimensionsto\\nhas suggested that simply adding retrieved docu-\\nconsider: theabilityoftheretrievalsystemto\\nidentifyrelevantandfocusedcontextpassages, mentstotheinputofastandardLMcanalsowork\\ntheabilityoftheLLMtoexploitsuchpassages well (Khattab et al., 2022; Ram et al., 2023; Shi\\nin a faithful way, or the quality of the gener- etal.,2023),thusmakingitpossibletouseretrieval-\\nationitself. With RAGAS,weputforwarda augmented strategies in combination with LLMs\\nsuiteofmetricswhichcanbeusedtoevaluate thatareonlyavailablethroughAPIs.\\nthese different dimensions without having to\\nWhile the usefulness of retrieval-augmented\\nrelyongroundtruthhumanannotations. We\\nstrategies is clear, their implementation requires\\npositthatsuchaframeworkcancruciallycon-\\nasignificantamountoftuning, astheoverallper-\\ntributetofasterevaluationcyclesofRAGarchi-\\ntectures, which is especially important given formance will be affected by the retrieval model,\\nthefastadoptionofLLMs. theconsideredcorpus,theLM,orthepromptfor-\\nmulation,amongothers. Automatedevaluationof\\n1 Introduction\\nretrieval-augmentedsystemsisthusparamount. In\\npractice,RAGsystemsareoftenevaluatedinterms\\nLanguage Models (LMs) capture a vast amount\\nofthelanguagemodellingtaskitself,i.e.bymea-\\nofknowledgeabouttheworld,whichallowsthem\\nsuringperplexityonsomereferencecorpus. How-\\nto answer questions without accessing any exter-\\never, such evaluations are not always predictive\\nnal sources. This idea of LMs as repositories of\\nofdownstreamperformance(Wangetal.,2023c).\\nknowledgeemergedshortlyaftertheintroduction\\nMoreover,thisevaluationstrategyreliesontheLM\\nof BERT (Devlin et al., 2019) and became more\\nprobabilities, which are not accessible for some\\nfirmly established with the introduction of ever\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\nlargerLMs(Robertsetal.,2020). Whilethemost\\ntionansweringisanothercommonevaluationtask,\\nrecent Large Language Models (LLMs) capture\\nbutusuallyonlydatasetswithshortextractivean-\\nenough knowledge to rival human performance\\nswersareconsidered,whichmaynotberepresen-\\nacrossawidevarietyofquestionansweringbench-\\ntativeofhowthesystemwillbeused.\\nmarks (Bubeck et al., 2023), the idea of using\\nToaddresstheseissues,inthispaperwepresent\\nLLMsasknowledgebasesstillhastwofundamen-\\ntallimitations. First,LLMsarenotabletoanswer\\nRAGAS1,aframeworkfortheautomatedassess-\\nquestions about events that have happened after\\n1RAGAS is available at https://github.com/\\ntheyweretrained. Second,eventhelargestmodels explodinggradients/ragas.\\n3202\\npeS\\n62\\n]LC.sc[\\n1v71251.9032:viXra\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9245f502-a66b-4997-9b91-5dbdacd7b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12052ecc-eb44-41d8-bdc2-f4d66a475097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
       "ShahulEs†,JithinJames†,LuisEspinosa-Anke∗♢,StevenSchockaert∗\n",
       "†ExplodingGradients\n",
       "∗CardiffNLP,CardiffUniversity,UnitedKingdom\n",
       "♢AMPLYFI,UnitedKingdom\n",
       "shahules786@gmail.com,jamesjithin97@gmail.com\n",
       "{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n",
       "Abstract struggletomemoriseknowledgethatisonlyrarely\n",
       "mentioned in the training corpus (Kandpal et al.,\n",
       "WeintroduceRAGAS(RetrievalAugmented\n",
       "2022;Mallenetal.,2023). Thestandardsolution\n",
       "Generation Assessment), a framework for\n",
       "to these issues is to rely on Retrieval Augmented\n",
       "reference-free evaluation of Retrieval Aug-\n",
       "Generation (RAG) (Lee et al., 2019; Lewis et al.,\n",
       "mented Generation (RAG) pipelines. RAG\n",
       "2020; Guu et al., 2020). Answering a question\n",
       "systems are composed of a retrieval and an\n",
       "LLM based generation module, and provide then essentially involves retrieving relevant pas-\n",
       "LLMswithknowledgefromareferencetextual sages from a corpus and feeding these passages,\n",
       "database,whichenablesthemtoactasanatu- alongwiththeoriginalquestion,totheLM.While\n",
       "rallanguagelayerbetweenauserandtextual initial approaches relied on specialised LMs for\n",
       "databases,reducingtheriskofhallucinations.\n",
       "retrieval-augmentedlanguagemodelling(Khandel-\n",
       "EvaluatingRAGarchitecturesis,however,chal-\n",
       "waletal.,2020;Borgeaudetal.,2022),recentwork\n",
       "lengingbecausethereareseveraldimensionsto\n",
       "has suggested that simply adding retrieved docu-\n",
       "consider: theabilityoftheretrievalsystemto\n",
       "identifyrelevantandfocusedcontextpassages, mentstotheinputofastandardLMcanalsowork\n",
       "theabilityoftheLLMtoexploitsuchpassages well (Khattab et al., 2022; Ram et al., 2023; Shi\n",
       "in a faithful way, or the quality of the gener- etal.,2023),thusmakingitpossibletouseretrieval-\n",
       "ationitself. With RAGAS,weputforwarda augmented strategies in combination with LLMs\n",
       "suiteofmetricswhichcanbeusedtoevaluate thatareonlyavailablethroughAPIs.\n",
       "these different dimensions without having to\n",
       "While the usefulness of retrieval-augmented\n",
       "relyongroundtruthhumanannotations. We\n",
       "strategies is clear, their implementation requires\n",
       "positthatsuchaframeworkcancruciallycon-\n",
       "asignificantamountoftuning, astheoverallper-\n",
       "tributetofasterevaluationcyclesofRAGarchi-\n",
       "tectures, which is especially important given formance will be affected by the retrieval model,\n",
       "thefastadoptionofLLMs. theconsideredcorpus,theLM,orthepromptfor-\n",
       "mulation,amongothers. Automatedevaluationof\n",
       "1 Introduction\n",
       "retrieval-augmentedsystemsisthusparamount. In\n",
       "practice,RAGsystemsareoftenevaluatedinterms\n",
       "Language Models (LMs) capture a vast amount\n",
       "ofthelanguagemodellingtaskitself,i.e.bymea-\n",
       "ofknowledgeabouttheworld,whichallowsthem\n",
       "suringperplexityonsomereferencecorpus. How-\n",
       "to answer questions without accessing any exter-\n",
       "ever, such evaluations are not always predictive\n",
       "nal sources. This idea of LMs as repositories of\n",
       "ofdownstreamperformance(Wangetal.,2023c).\n",
       "knowledgeemergedshortlyaftertheintroduction\n",
       "Moreover,thisevaluationstrategyreliesontheLM\n",
       "of BERT (Devlin et al., 2019) and became more\n",
       "probabilities, which are not accessible for some\n",
       "firmly established with the introduction of ever\n",
       "closed models (e.g. ChatGPT and GPT-4). Ques-\n",
       "largerLMs(Robertsetal.,2020). Whilethemost\n",
       "tionansweringisanothercommonevaluationtask,\n",
       "recent Large Language Models (LLMs) capture\n",
       "butusuallyonlydatasetswithshortextractivean-\n",
       "enough knowledge to rival human performance\n",
       "swersareconsidered,whichmaynotberepresen-\n",
       "acrossawidevarietyofquestionansweringbench-\n",
       "tativeofhowthesystemwillbeused.\n",
       "marks (Bubeck et al., 2023), the idea of using\n",
       "Toaddresstheseissues,inthispaperwepresent\n",
       "LLMsasknowledgebasesstillhastwofundamen-\n",
       "tallimitations. First,LLMsarenotabletoanswer\n",
       "RAGAS1,aframeworkfortheautomatedassess-\n",
       "questions about events that have happened after\n",
       "1RAGAS is available at https://github.com/\n",
       "theyweretrained. Second,eventhelargestmodels explodinggradients/ragas.\n",
       "3202\n",
       "peS\n",
       "62\n",
       "]LC.sc[\n",
       "1v71251.9032:viXra\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab932c-e08c-4641-8d53-aa7a82670909",
   "metadata": {},
   "source": [
    "## Step 3: Chunk Documents Strategically\n",
    "Leverage Streamlit’s file uploader to select a local PDF. Use PDFPlumberLoader to extract text efficiently without manual parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69275db8-e589-4047-adf6-989292a224cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736/1866683925.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n",
      "/tmp/ipykernel_1736/1866683925.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "# Split text into semantic chunks  \n",
    "text_splitter = SemanticChunker(HuggingFaceEmbeddings())  \n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5a80d1f-8f36-4e10-b4de-5d81ff70dfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1253afb3-22ec-4874-862b-2c7885ec9964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './documents/2309.15217v1.pdf', 'file_path': './documents/2309.15217v1.pdf', 'page': 0, 'total_pages': 8, 'Author': '', 'CreationDate': 'D:20230928011700Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20230928011700Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='RAGAS: Automated Evaluation of Retrieval Augmented Generation\\nShahulEs†,JithinJames†,LuisEspinosa-Anke∗♢,StevenSchockaert∗\\n†ExplodingGradients\\n∗CardiffNLP,CardiffUniversity,UnitedKingdom\\n♢AMPLYFI,UnitedKingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract struggletomemoriseknowledgethatisonlyrarely\\nmentioned in the training corpus (Kandpal et al.,\\nWeintroduceRAGAS(RetrievalAugmented\\n2022;Mallenetal.,2023). Thestandardsolution\\nGeneration Assessment), a framework for\\nto these issues is to rely on Retrieval Augmented\\nreference-free evaluation of Retrieval Aug-\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\nmented Generation (RAG) pipelines. RAG\\n2020; Guu et al., 2020). Answering a question\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide then essentially involves retrieving relevant pas-\\nLLMswithknowledgefromareferencetextual sages from a corpus and feeding these passages,\\ndatabase,whichenablesthemtoactasanatu- alongwiththeoriginalquestion,totheLM.While\\nrallanguagelayerbetweenauserandtextual initial approaches relied on specialised LMs for\\ndatabases,reducingtheriskofhallucinations. retrieval-augmentedlanguagemodelling(Khandel-\\nEvaluatingRAGarchitecturesis,however,chal-\\nwaletal.,2020;Borgeaudetal.,2022),recentwork\\nlengingbecausethereareseveraldimensionsto\\nhas suggested that simply adding retrieved docu-\\nconsider: theabilityoftheretrievalsystemto\\nidentifyrelevantandfocusedcontextpassages, mentstotheinputofastandardLMcanalsowork\\ntheabilityoftheLLMtoexploitsuchpassages well (Khattab et al., 2022; Ram et al., 2023; Shi\\nin a faithful way, or the quality of the gener- etal.,2023),thusmakingitpossibletouseretrieval-\\nationitself. With RAGAS,weputforwarda augmented strategies in combination with LLMs\\nsuiteofmetricswhichcanbeusedtoevaluate thatareonlyavailablethroughAPIs. these different dimensions without having to\\nWhile the usefulness of retrieval-augmented\\nrelyongroundtruthhumanannotations. We\\nstrategies is clear, their implementation requires\\npositthatsuchaframeworkcancruciallycon-\\nasignificantamountoftuning, astheoverallper-\\ntributetofasterevaluationcyclesofRAGarchi-\\ntectures, which is especially important given formance will be affected by the retrieval model,\\nthefastadoptionofLLMs. theconsideredcorpus,theLM,orthepromptfor-\\nmulation,amongothers. Automatedevaluationof\\n1 Introduction\\nretrieval-augmentedsystemsisthusparamount. In\\npractice,RAGsystemsareoftenevaluatedinterms\\nLanguage Models (LMs) capture a vast amount\\nofthelanguagemodellingtaskitself,i.e.bymea-\\nofknowledgeabouttheworld,whichallowsthem\\nsuringperplexityonsomereferencecorpus. How-\\nto answer questions without accessing any exter-\\never, such evaluations are not always predictive\\nnal sources. This idea of LMs as repositories of\\nofdownstreamperformance(Wangetal.,2023c). knowledgeemergedshortlyaftertheintroduction\\nMoreover,thisevaluationstrategyreliesontheLM\\nof BERT (Devlin et al., 2019) and became more\\nprobabilities, which are not accessible for some\\nfirmly established with the introduction of ever\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\nlargerLMs(Robertsetal.,2020). Whilethemost\\ntionansweringisanothercommonevaluationtask,\\nrecent Large Language Models (LLMs) capture\\nbutusuallyonlydatasetswithshortextractivean-\\nenough knowledge to rival human performance\\nswersareconsidered,whichmaynotberepresen-\\nacrossawidevarietyofquestionansweringbench-\\ntativeofhowthesystemwillbeused. marks (Bubeck et al., 2023), the idea of using\\nToaddresstheseissues,inthispaperwepresent\\nLLMsasknowledgebasesstillhastwofundamen-\\ntallimitations.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121000b1-a7b2-452c-b121-948c0006d1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
       "ShahulEs†,JithinJames†,LuisEspinosa-Anke∗♢,StevenSchockaert∗\n",
       "†ExplodingGradients\n",
       "∗CardiffNLP,CardiffUniversity,UnitedKingdom\n",
       "♢AMPLYFI,UnitedKingdom\n",
       "shahules786@gmail.com,jamesjithin97@gmail.com\n",
       "{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n",
       "Abstract struggletomemoriseknowledgethatisonlyrarely\n",
       "mentioned in the training corpus (Kandpal et al.,\n",
       "WeintroduceRAGAS(RetrievalAugmented\n",
       "2022;Mallenetal.,2023). Thestandardsolution\n",
       "Generation Assessment), a framework for\n",
       "to these issues is to rely on Retrieval Augmented\n",
       "reference-free evaluation of Retrieval Aug-\n",
       "Generation (RAG) (Lee et al., 2019; Lewis et al.,\n",
       "mented Generation (RAG) pipelines. RAG\n",
       "2020; Guu et al., 2020). Answering a question\n",
       "systems are composed of a retrieval and an\n",
       "LLM based generation module, and provide then essentially involves retrieving relevant pas-\n",
       "LLMswithknowledgefromareferencetextual sages from a corpus and feeding these passages,\n",
       "database,whichenablesthemtoactasanatu- alongwiththeoriginalquestion,totheLM.While\n",
       "rallanguagelayerbetweenauserandtextual initial approaches relied on specialised LMs for\n",
       "databases,reducingtheriskofhallucinations. retrieval-augmentedlanguagemodelling(Khandel-\n",
       "EvaluatingRAGarchitecturesis,however,chal-\n",
       "waletal.,2020;Borgeaudetal.,2022),recentwork\n",
       "lengingbecausethereareseveraldimensionsto\n",
       "has suggested that simply adding retrieved docu-\n",
       "consider: theabilityoftheretrievalsystemto\n",
       "identifyrelevantandfocusedcontextpassages, mentstotheinputofastandardLMcanalsowork\n",
       "theabilityoftheLLMtoexploitsuchpassages well (Khattab et al., 2022; Ram et al., 2023; Shi\n",
       "in a faithful way, or the quality of the gener- etal.,2023),thusmakingitpossibletouseretrieval-\n",
       "ationitself. With RAGAS,weputforwarda augmented strategies in combination with LLMs\n",
       "suiteofmetricswhichcanbeusedtoevaluate thatareonlyavailablethroughAPIs. these different dimensions without having to\n",
       "While the usefulness of retrieval-augmented\n",
       "relyongroundtruthhumanannotations. We\n",
       "strategies is clear, their implementation requires\n",
       "positthatsuchaframeworkcancruciallycon-\n",
       "asignificantamountoftuning, astheoverallper-\n",
       "tributetofasterevaluationcyclesofRAGarchi-\n",
       "tectures, which is especially important given formance will be affected by the retrieval model,\n",
       "thefastadoptionofLLMs. theconsideredcorpus,theLM,orthepromptfor-\n",
       "mulation,amongothers. Automatedevaluationof\n",
       "1 Introduction\n",
       "retrieval-augmentedsystemsisthusparamount. In\n",
       "practice,RAGsystemsareoftenevaluatedinterms\n",
       "Language Models (LMs) capture a vast amount\n",
       "ofthelanguagemodellingtaskitself,i.e.bymea-\n",
       "ofknowledgeabouttheworld,whichallowsthem\n",
       "suringperplexityonsomereferencecorpus. How-\n",
       "to answer questions without accessing any exter-\n",
       "ever, such evaluations are not always predictive\n",
       "nal sources. This idea of LMs as repositories of\n",
       "ofdownstreamperformance(Wangetal.,2023c). knowledgeemergedshortlyaftertheintroduction\n",
       "Moreover,thisevaluationstrategyreliesontheLM\n",
       "of BERT (Devlin et al., 2019) and became more\n",
       "probabilities, which are not accessible for some\n",
       "firmly established with the introduction of ever\n",
       "closed models (e.g. ChatGPT and GPT-4). Ques-\n",
       "largerLMs(Robertsetal.,2020). Whilethemost\n",
       "tionansweringisanothercommonevaluationtask,\n",
       "recent Large Language Models (LLMs) capture\n",
       "butusuallyonlydatasetswithshortextractivean-\n",
       "enough knowledge to rival human performance\n",
       "swersareconsidered,whichmaynotberepresen-\n",
       "acrossawidevarietyofquestionansweringbench-\n",
       "tativeofhowthesystemwillbeused. marks (Bubeck et al., 2023), the idea of using\n",
       "Toaddresstheseissues,inthispaperwepresent\n",
       "LLMsasknowledgebasesstillhastwofundamen-\n",
       "tallimitations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2ae44-2729-4691-b9d7-c4e69714fdcb",
   "metadata": {},
   "source": [
    "## Step 4: Create a Searchable Knowledge Base\n",
    "Generate vector embeddings for the chunks and store them in a FAISS index.\n",
    "\n",
    "- Embeddings allow fast, contextually relevant searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468fc476-d446-4a6f-82c9-ccae38ed8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736/465016057.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings  \n",
    "embeddings = HuggingFaceEmbeddings()  \n",
    "vector_store = FAISS.from_documents(documents, embeddings)  \n",
    "\n",
    "# Connect retriever  \n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})  # Fetch top 3 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2e785-d30a-44fe-97e5-63d2a6a9f84c",
   "metadata": {},
   "source": [
    "## Step 5: Configure DeepSeek R1\n",
    "Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B** model.\n",
    "\n",
    "- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18ebc7df-3a85-40fc-b1f5-dc87f75b2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"deepseek-r1:1.5b\")  # Our 1.5B parameter model  \n",
    "\n",
    "\n",
    "# Craft the prompt template\n",
    "# 3. Keep answers under 4 sentences.  \n",
    "prompt = \"\"\"  \n",
    "1. Use ONLY the context below.  \n",
    "2. If unsure, say \"I don’t know\".  \n",
    "\n",
    "Context: {context}  \n",
    "\n",
    "Question: {question}  \n",
    "\n",
    "Answer:  \n",
    "\"\"\"  \n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60820b82-604c-4382-9918-bbf4fd4a09b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='  \\n1. Use ONLY the context below.  \\n2. If unsure, say \"I don’t know\".  \\n\\nContext: {context}  \\n\\nQuestion: {question}  \\n\\nAnswer:  \\n')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_CHAIN_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9ba9a-b724-40a7-906e-706c8e43dbbe",
   "metadata": {},
   "source": [
    "## Step 6: Assemble the RAG Chain\n",
    "Integrate uploading, chunking, and retrieval into a cohesive pipeline.\n",
    "\n",
    "- This approach gives the model verified context, enhancing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "266a7742-3527-4ec6-a3dc-0464c6ccda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain, RetrievalQA\n",
    "\n",
    "# Chain 1: Generate answers  \n",
    "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  \n",
    "\n",
    "# Chain 2: Combine document chunks  \n",
    "document_prompt = PromptTemplate(  \n",
    "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",  \n",
    "    input_variables=[\"page_content\", \"source\"]  \n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "\n",
    "# Final RAG pipeline  \n",
    "qa = RetrievalQA(  \n",
    "    combine_documents_chain=StuffDocumentsChain(  \n",
    "        llm_chain=llm_chain,  \n",
    "        document_prompt=document_prompt,\n",
    "        document_variable_name=document_variable_name\n",
    "    ),  \n",
    "    retriever=retriever  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "688af120-ef58-45fb-98e6-92442fe044f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  Define LLMs, RAG, and Agents\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter your question: \")\n",
    "response = qa(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a382b656-1deb-4704-bdd3-bb182406ece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to define LLMs, RAG, and Agents based on the context provided. Let me start by reading through each section carefully.\n",
      "\n",
      "First, there's a mention of Amos Azaria and Tom Mitchell in 2023. Their work is about the internal state of an LLM when lying. The source is from ./documents/2309.15217v1.pdf. I know that an LLM stands for Large Language Model, which is a type of AI designed to understand and generate human language.\n",
      "\n",
      "Next, there's information about RAGAS. It says LLMs aren't great at answering the \"What has happened\" questions, but they can answer other types of questions. The source links to a GitHub repository where LLMs are trained on datasets. They're trained and then used in various ways, like scoring systems. I remember hearing about RAGAS before; it's a framework that allows multiple models to share the same evaluation metrics.\n",
      "\n",
      "Then there's a section talking about the origins of LLMs. It mentions that these models have \"exploding gradients\" and are \"extremely large.\" They're also used in automated retrieval for questions after 1 year. I think this refers to how LLMs can retrieve information from documents, especially since they've been trained on a vast amount of data.\n",
      "\n",
      "Finally, the context discusses different aspects of evaluating text generation systems. It mentions RAGS as an evaluation framework that integrates with LLMs. The idea is to assess facts versus fictions by using prompts. There's also talk about detecting hallucinations—when answers might be misleading or incorrect—and comparing faithfulness scores. This ties into how RAGA works, ensuring accurate evaluations.\n",
      "\n",
      "Putting it all together: LLMs are AI models that can process language. RAGAS is a system that evaluates text responses, considering factual vs. fictional answers and using prompts to assess accuracy. Agents likely refer to the users or systems that interact with these models.\n",
      "</think>\n",
      "\n",
      "LLMs (Large Language Models) are advanced AI systems designed to generate and understand human language, capable of processing vast amounts of data and context.\n",
      "\n",
      "RAGAS is a framework for evaluating text responses, focusing on factual versus fictional answers and using prompts to assess accuracy. It integrates with LLMs to ensure evaluations reflect truthfulness and correctness.\n",
      "\n",
      "Agents in this context likely refer to systems or users that interact with these models, leveraging their capabilities for tasks like retrieval and evaluation of text.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65ae3e38-b081-47c1-8cf0-4914e0021a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I need to define LLMs, RAG, and Agents based on the context provided. Let me start by reading through each section carefully.\n",
       "\n",
       "First, there's a mention of Amos Azaria and Tom Mitchell in 2023. Their work is about the internal state of an LLM when lying. The source is from ./documents/2309.15217v1.pdf. I know that an LLM stands for Large Language Model, which is a type of AI designed to understand and generate human language.\n",
       "\n",
       "Next, there's information about RAGAS. It says LLMs aren't great at answering the \"What has happened\" questions, but they can answer other types of questions. The source links to a GitHub repository where LLMs are trained on datasets. They're trained and then used in various ways, like scoring systems. I remember hearing about RAGAS before; it's a framework that allows multiple models to share the same evaluation metrics.\n",
       "\n",
       "Then there's a section talking about the origins of LLMs. It mentions that these models have \"exploding gradients\" and are \"extremely large.\" They're also used in automated retrieval for questions after 1 year. I think this refers to how LLMs can retrieve information from documents, especially since they've been trained on a vast amount of data.\n",
       "\n",
       "Finally, the context discusses different aspects of evaluating text generation systems. It mentions RAGS as an evaluation framework that integrates with LLMs. The idea is to assess facts versus fictions by using prompts. There's also talk about detecting hallucinations—when answers might be misleading or incorrect—and comparing faithfulness scores. This ties into how RAGA works, ensuring accurate evaluations.\n",
       "\n",
       "Putting it all together: LLMs are AI models that can process language. RAGAS is a system that evaluates text responses, considering factual vs. fictional answers and using prompts to assess accuracy. Agents likely refer to the users or systems that interact with these models.\n",
       "</think>\n",
       "\n",
       "LLMs (Large Language Models) are advanced AI systems designed to generate and understand human language, capable of processing vast amounts of data and context.\n",
       "\n",
       "RAGAS is a framework for evaluating text responses, focusing on factual versus fictional answers and using prompts to assess accuracy. It integrates with LLMs to ensure evaluations reflect truthfulness and correctness.\n",
       "\n",
       "Agents in this context likely refer to systems or users that interact with these models, leveraging their capabilities for tasks like retrieval and evaluation of text."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response[\"result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099ee1c-2e2f-40f1-8794-44bc46d2f7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
